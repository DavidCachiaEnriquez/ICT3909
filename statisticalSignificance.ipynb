{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25752\\3468275910.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateResults(frame):\n",
    "    results = {\"Accuracy\":[], \"F1 Score\":[]}    \n",
    "    for _, row in frame.iterrows():\n",
    "        matrixString = row[\"confusionMatrix\"]\n",
    "\n",
    "        tempString = \"\"\n",
    "        for char in matrixString:\n",
    "            tempString += char\n",
    "            if char == \"[\": tempString += \" \"\n",
    "        matrixString = tempString.replace(\"[\", \"\").replace(\"]\", \"\").split()        \n",
    "\n",
    "        trueConfMatrix = [[matrixString[0], matrixString[1]], [matrixString[2], matrixString[3]]]   \n",
    "        TP, FP, FN, TN = float(trueConfMatrix[0][0]), float(trueConfMatrix[0][1]), float(trueConfMatrix[1][0]), float(trueConfMatrix[1][1])\n",
    "\n",
    "        accuracy = (TP + TN)/(TP + FP + FN + TN) if (TP + FP + FN + TN) != 0 else 0\n",
    "        precision = (TP)/(TP + FP) if (TP + FP) != 0 else 0\n",
    "        recall = (TP)/(TP + FN) if (TP + FN) != 0 else 0\n",
    "        f1Score = 2 * ((precision*recall)/(precision+recall)) if (precision+recall) != 0 else 0\n",
    "        \n",
    "        results[\"Accuracy\"].append(accuracy)\n",
    "        results[\"F1 Score\"].append(f1Score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFrame = []\n",
    "\n",
    "# Different Parameters\n",
    "inputs = [\"Audio\", \"Visual\", \"Phys\", \"All\"]\n",
    "targets = [\"Arousal\", \"Valence\"]\n",
    "baseEstimator = [\"BLR\", \"RF\", \"NN\"]\n",
    "labelledCount = ['4', \"8\", \"12\"]\n",
    "\n",
    "supervisedModels = [\"Binary Logistic Regression\", \"Random Forest\", \"Neural Network\"]\n",
    "semiSupervisedModels = [\"Co-Training\", \"Tri-Training\", \"SSGMM\", \"Assemble\", \"SemiBoost\"]\n",
    "\n",
    "# To get Supervised\n",
    "for model in supervisedModels:\n",
    "    for input in inputs:\n",
    "        for target in targets:\n",
    "            \n",
    "            baseCode = None\n",
    "            if model == 'Binary Logistic Regression': baseCode = \"BLR\"\n",
    "            elif model == 'Random Forest': baseCode = \"RF\"\n",
    "            elif model == 'Neural Network': baseCode = \"NN\"\n",
    "            \n",
    "            # Supervised\n",
    "            supervisedTest = baseCode + \"_\" + input + \"_\" + target\n",
    "\n",
    "            folderLocation = \"Datasets/RECOLA/Supervised Models/\" + model + \"/Per Fold Results\"\n",
    "            listOfFiles = os.listdir(folderLocation)\n",
    "\n",
    "            fileName = None\n",
    "            for file in listOfFiles:\n",
    "                if input in file and target in file:\n",
    "                    fileName = file\n",
    "            fileLocation = folderLocation + \"/\" + fileName\n",
    "            superFrame = pd.read_csv(fileLocation); superFrame = superFrame.drop(columns=[\"trainAccuracy\", \"trainPrecision\", \"testAccuracy\", \"testPrecision\"])\n",
    "\n",
    "            supervisedResults = calculateResults(superFrame)\n",
    "\n",
    "            # Semi-Supervised\n",
    "            for semiModel in semiSupervisedModels:\n",
    "                for count in labelledCount:\n",
    "                    \n",
    "                    if baseCode == \"NN\" and semiModel == \"Assemble\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        semiSupervisedTest = semiModel + \"_\" + count\n",
    "\n",
    "                        semiFolderLocation = \"Datasets/RECOLA/Semi-Supervised Models/\" + semiModel + \"/Per Fold Results\"\n",
    "                        listOfSemiFiles = os.listdir(semiFolderLocation)\n",
    "\n",
    "                        semiFileName = None\n",
    "                        for file in listOfSemiFiles:\n",
    "                            if input in file and target in file and baseCode in file and count in file:\n",
    "                                semiFileName = file\n",
    "                            elif semiModel == \"SSGMM\":\n",
    "                                if input in file and target in file and count in file:\n",
    "                                    semiFileName = file\n",
    "                        \n",
    "                        semiFileLocation = semiFolderLocation + \"/\" + semiFileName\n",
    "                        semiSuperFrame = pd.read_csv(semiFileLocation); semiSuperFrame = semiSuperFrame.drop(columns=[\"trainAccuracy\", \"trainPrecision\", \"testAccuracy\", \"testPrecision\"])\n",
    "\n",
    "                        semiSupervisedResults = calculateResults(semiSuperFrame)\n",
    "\n",
    "                        # Create Entry\n",
    "                        testName = supervisedTest + \" - \" + semiSupervisedTest\n",
    "\n",
    "                        tempDict = {}\n",
    "                        tempDict[\"Test Name\"] = testName\n",
    "                        tempDict[\"Accuracy\"] = scipy.stats.ttest_ind(supervisedResults[\"Accuracy\"], semiSupervisedResults[\"Accuracy\"])[1]\n",
    "                        tempDict[\"F1 Score\"] = scipy.stats.ttest_ind(supervisedResults[\"F1 Score\"], semiSupervisedResults[\"F1 Score\"])[1]\n",
    "                        \n",
    "                        tempDict[\"Significance: Accuracy\"] = 1 if tempDict[\"Accuracy\"] > 0.05 else 0\n",
    "                        tempDict[\"Significance: F1 Score\"] = 1 if tempDict[\"F1 Score\"] > 0.05 else 0\n",
    "                        finalFrame.append(tempDict)\n",
    "\n",
    "finalFrame = pd.DataFrame(finalFrame)\n",
    "finalFrame.to_csv(\"Results/RECOLA_Significance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFrame = []\n",
    "\n",
    "# Different Parameters\n",
    "games = [\"TopDown\", \"Shootout\", \"Heist!\"]\n",
    "baseEstimator = [\"BLR\", \"RF\", \"NN\"]\n",
    "labelledCount = ['4', \"8\", \"12\"]\n",
    "\n",
    "supervisedModels = [\"Binary Logistic Regression\", \"Random Forest\", \"Neural Network\"]\n",
    "semiSupervisedModels = [\"Co-Training\", \"Tri-Training\", \"SSGMM\", \"Assemble\", \"SemiBoost\"]\n",
    "\n",
    "# To get Supervised\n",
    "for model in supervisedModels:\n",
    "    for game in games:\n",
    "            \n",
    "        baseCode = None\n",
    "        if model == 'Binary Logistic Regression': baseCode = \"BLR\"\n",
    "        elif model == 'Random Forest': baseCode = \"RF\"\n",
    "        elif model == 'Neural Network': baseCode = \"NN\"\n",
    "        \n",
    "        # Supervised\n",
    "        supervisedTest = baseCode + \"_\" + game\n",
    "\n",
    "        folderLocation = \"Datasets/AGAIN/Supervised Models/\" + game + \"/\" + model + \"/Per Fold Results\"\n",
    "        listOfFiles = os.listdir(folderLocation)\n",
    "        \n",
    "        fileName = None\n",
    "        for file in listOfFiles:\n",
    "            if game in file:\n",
    "                fileName = file\n",
    "        fileLocation = folderLocation + \"/\" + fileName\n",
    "        superFrame = pd.read_csv(fileLocation); superFrame = superFrame.drop(columns=[\"trainAccuracy\", \"trainPrecision\", \"testAccuracy\", \"testPrecision\"])\n",
    "\n",
    "        supervisedResults = calculateResults(superFrame)\n",
    "\n",
    "        # Semi-Supervised\n",
    "        for semiModel in semiSupervisedModels:\n",
    "            for count in labelledCount:\n",
    "                \n",
    "                if baseCode == \"NN\" and semiModel == \"Assemble\":\n",
    "                    pass\n",
    "                else:\n",
    "                    semiSupervisedTest = semiModel + \"_\" + count\n",
    "\n",
    "                    semiFolderLocation = \"Datasets/AGAIN/Semi-Supervised Models/\" + game + \"/\" + semiModel + \"/Per Fold Results\"\n",
    "                    listOfSemiFiles = os.listdir(semiFolderLocation)\n",
    "\n",
    "                    semiFileName = None\n",
    "                    for file in listOfSemiFiles:\n",
    "                        if game in file and baseCode in file and count in file:\n",
    "                            semiFileName = file\n",
    "                        elif semiModel == \"SSGMM\":\n",
    "                            if game in file and count in file:\n",
    "                                semiFileName = file\n",
    "                    \n",
    "                    semiFileLocation = semiFolderLocation + \"/\" + semiFileName\n",
    "                    semiSuperFrame = pd.read_csv(semiFileLocation); semiSuperFrame = semiSuperFrame.drop(columns=[\"trainAccuracy\", \"trainPrecision\", \"testAccuracy\", \"testPrecision\"])\n",
    "\n",
    "                    semiSupervisedResults = calculateResults(semiSuperFrame)\n",
    "\n",
    "                    # Create Entry\n",
    "                    testName = supervisedTest + \" - \" + semiSupervisedTest\n",
    "\n",
    "                    tempDict = {}\n",
    "                    tempDict[\"Test Name\"] = testName\n",
    "                    tempDict[\"Accuracy\"] = scipy.stats.ttest_ind(supervisedResults[\"Accuracy\"], semiSupervisedResults[\"Accuracy\"])[1]\n",
    "                    tempDict[\"F1 Score\"] = scipy.stats.ttest_ind(supervisedResults[\"F1 Score\"], semiSupervisedResults[\"F1 Score\"])[1]\n",
    "                    \n",
    "                    tempDict[\"Significance: Accuracy\"] = 1 if tempDict[\"Accuracy\"] > 0.05 else 0\n",
    "                    tempDict[\"Significance: F1 Score\"] = 1 if tempDict[\"F1 Score\"] > 0.05 else 0\n",
    "                    finalFrame.append(tempDict)\n",
    "\n",
    "finalFrame = pd.DataFrame(finalFrame)\n",
    "finalFrame.to_csv(\"Results/AGAIN_Significance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countChecker(list):\n",
    "    totalLength = len(list)\n",
    "    passCounter = 0\n",
    "    for row in list:\n",
    "        if row == 1: passCounter += 1\n",
    "\n",
    "    return passCounter, totalLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCalculator(nameList, binaryList):\n",
    "    totalLength = len(binaryList)\n",
    "\n",
    "    overallCount = 0\n",
    "    CoTrainingCount, TriTrainingCount, SSGMMCount, AssembleCount, SemiBoostCount = 0, 0, 0, 0, 0\n",
    "    CoTrainingTotal, TriTrainingTotal, SSGMMTotal, AssembleTotal, SemiBoostTotal = 0, 0, 0, 0, 0\n",
    "    for index in range(totalLength):\n",
    "        if binaryList[index] == 1:\n",
    "            overallCount += 1\n",
    "            if \"Co-Training\" in nameList[index]: CoTrainingCount += 1\n",
    "            if \"Tri-Training\" in nameList[index]: TriTrainingCount += 1\n",
    "            if \"SSGMM\" in nameList[index]: SSGMMCount += 1\n",
    "            if \"Assemble\" in nameList[index]: AssembleCount += 1\n",
    "            if \"SemiBoost\" in nameList[index]: SemiBoostCount += 1\n",
    "        \n",
    "        if \"Co-Training\" in nameList[index]: CoTrainingTotal += 1\n",
    "        if \"Tri-Training\" in nameList[index]: TriTrainingTotal += 1\n",
    "        if \"SSGMM\" in nameList[index]: SSGMMTotal += 1\n",
    "        if \"Assemble\" in nameList[index]: AssembleTotal += 1\n",
    "        if \"SemiBoost\" in nameList[index]: SemiBoostTotal += 1\n",
    "\n",
    "\n",
    "    return totalLength, overallCount, CoTrainingCount, TriTrainingCount, SSGMMCount, AssembleCount, SemiBoostCount, CoTrainingTotal, TriTrainingTotal, SSGMMTotal, AssembleTotal, SemiBoostTotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R E C O L A: Statistical Significance Evaluation\n",
      "Overall Significance: 271/336 (80.65%)\n",
      "Co-Training Significance: 68/72 (94.44%)\n",
      "Tri-Training Significance: 71/72 (98.61%)\n",
      "SSGMM Significance: 38/72 (52.78%)\n",
      "Assemble Significance: 43/48 (89.58%)\n",
      "SemiBoost Significance: 51/72 (70.83%)\n",
      "\n",
      "\n",
      "A G A I N: Statistical Significance Evaluation\n",
      "Overall Significance: 80/126 (63.49%)\n",
      "Co-Training Significance: 22/27 (81.48%)\n",
      "Tri-Training Significance: 26/27 (96.3%)\n",
      "SSGMM Significance: 0/27 (0.0%)\n",
      "Assemble Significance: 18/18 (100.0%)\n",
      "SemiBoost Significance: 14/27 (51.85%)\n"
     ]
    }
   ],
   "source": [
    "print(\"R E C O L A: Statistical Significance Evaluation\"); frame = pd.read_csv(\"Results/RECOLA_Significance.csv\")\n",
    "\n",
    "totalLength, overallCount, CoTrainingCount, TriTrainingCount, SSGMMCount, AssembleCount, SemiBoostCount, CoTrainingTotal, TriTrainingTotal, SSGMMTotal, AssembleTotal, SemiBoostTotal = countCalculator(list(frame[\"Test Name\"]), list(frame[\"Significance: Accuracy\"]))\n",
    "print(\"Overall Significance: \" + str(overallCount) + \"/\" + str(totalLength) + \" (\" + str(round((overallCount/totalLength)*100, 2)) + \"%)\")\n",
    "print(\"Co-Training Significance: \" + str(CoTrainingCount) + \"/\" + str(CoTrainingTotal) + \" (\" + str(round((CoTrainingCount/CoTrainingTotal)*100, 2)) + \"%)\")\n",
    "print(\"Tri-Training Significance: \" + str(TriTrainingCount) + \"/\" + str(TriTrainingTotal) + \" (\" + str(round((TriTrainingCount/TriTrainingTotal)*100, 2)) + \"%)\")\n",
    "print(\"SSGMM Significance: \" + str(SSGMMCount) + \"/\" + str(SSGMMTotal) + \" (\" + str(round((SSGMMCount/SSGMMTotal)*100, 2)) + \"%)\")\n",
    "print(\"Assemble Significance: \" + str(AssembleCount) + \"/\" + str(AssembleTotal) + \" (\" + str(round((AssembleCount/AssembleTotal)*100, 2)) + \"%)\")\n",
    "print(\"SemiBoost Significance: \" + str(SemiBoostCount) + \"/\" + str(SemiBoostTotal) + \" (\" + str(round((SemiBoostCount/SemiBoostTotal)*100, 2)) + \"%)\")\n",
    "\n",
    "print(\"\\n\\nA G A I N: Statistical Significance Evaluation\"); frame = pd.read_csv(\"Results/AGAIN_Significance.csv\")\n",
    "\n",
    "totalLength, overallCount, CoTrainingCount, TriTrainingCount, SSGMMCount, AssembleCount, SemiBoostCount, CoTrainingTotal, TriTrainingTotal, SSGMMTotal, AssembleTotal, SemiBoostTotal = countCalculator(list(frame[\"Test Name\"]), list(frame[\"Significance: Accuracy\"]))\n",
    "print(\"Overall Significance: \" + str(overallCount) + \"/\" + str(totalLength) + \" (\" + str(round((overallCount/totalLength)*100, 2)) + \"%)\")\n",
    "print(\"Co-Training Significance: \" + str(CoTrainingCount) + \"/\" + str(CoTrainingTotal) + \" (\" + str(round((CoTrainingCount/CoTrainingTotal)*100, 2)) + \"%)\")\n",
    "print(\"Tri-Training Significance: \" + str(TriTrainingCount) + \"/\" + str(TriTrainingTotal) + \" (\" + str(round((TriTrainingCount/TriTrainingTotal)*100, 2)) + \"%)\")\n",
    "print(\"SSGMM Significance: \" + str(SSGMMCount) + \"/\" + str(SSGMMTotal) + \" (\" + str(round((SSGMMCount/SSGMMTotal)*100, 2)) + \"%)\")\n",
    "print(\"Assemble Significance: \" + str(AssembleCount) + \"/\" + str(AssembleTotal) + \" (\" + str(round((AssembleCount/AssembleTotal)*100, 2)) + \"%)\")\n",
    "print(\"SemiBoost Significance: \" + str(SemiBoostCount) + \"/\" + str(SemiBoostTotal) + \" (\" + str(round((SemiBoostCount/SemiBoostTotal)*100, 2)) + \"%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
